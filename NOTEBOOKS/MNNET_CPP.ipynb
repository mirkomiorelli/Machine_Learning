{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Multilayer Neural Network from scratch using C++</h1>\n",
    "\n",
    "Here I implemented a multilayer neural network in c++ using the wrapper pybind11 to create a python library (the library is imported using <tt>import nnet</tt>).\n",
    "\n",
    "After importing the library I test the network on the iris dataset against an equivalent architecture built with the keras interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# Add library path to system libraries\n",
    "import sys\n",
    "sys.path.append('../CPP/MNNET/')\n",
    "import nnet\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3            4\n",
      "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
      "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
      "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
      "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
      "4  5.0  3.6  1.4  0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Read dataset in dataframe\n",
    "df = pd.read_csv('../DATASETS/iris_dataset.csv',header=None).dropna()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Design matrix and target vector\n",
    "X = df.loc[:,:3].values\n",
    "y = df.loc[:,4].values\n",
    "# Scale design matrix (X) entries from 0 to 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(copy=True, feature_range=(0,1))\n",
    "X = scaler.fit_transform(X)\n",
    "# Encode target vector (y) with dummy variables (one-hot encoding)\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to shuffle data\n",
    "def _shuffle(X,y):\n",
    "    idx = np.arange(len(y))\n",
    "    np.random.shuffle(idx)    \n",
    "    return(X[idx,:], y[idx,:])\n",
    "# Function to split data set into training and cross-validation (or test) data sets\n",
    "# p_train is the percentage of the data set used as training (default is 80%)\n",
    "def _split(X,y,p_train=0.8):\n",
    "    idx = int(len(y) * p_train)\n",
    "    Xtrain = X[:idx,:]\n",
    "    ytrain = y[:idx,:]\n",
    "    Xcv = X[idx:,:]\n",
    "    ycv = y[idx:,:]\n",
    "    return Xtrain, ytrain, Xcv, ycv\n",
    "\n",
    "# Shuffle data randomly\n",
    "np.random.seed(12365)\n",
    "X,y = _shuffle(X,y)\n",
    "# Split in training and cross validation sets\n",
    "Xtrain, ytrain, Xcv, ycv = _split(X,y)\n",
    "\n",
    "# Get accuracy on a given dataset\n",
    "def _test(nnet, X, y, verbose=False):\n",
    "    ypred = []\n",
    "    for x in X:\n",
    "        temp = np.argmax(nnet.predict(x,[]))\n",
    "        ypred.append(temp)\n",
    "    y = [np.argmax(i) for i in y]\n",
    "    success = np.nansum([1 for i in range(len(y)) if y[i] == ypred[i]])\n",
    "    if verbose:\n",
    "        print(\"Accuracy on dataset: %5.2f %%\" % (100*np.divide(success,len(ypred))))\n",
    "    return 100*np.divide(success,len(ypred)), ypred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Custom implementation in C++</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular dataset we will use three hidden layers with sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset: 97.50 %\n",
      "[[46  0  0]\n",
      " [ 0 35  2]\n",
      " [ 0  1 36]]\n",
      "Accuracy on dataset: 96.67 %\n",
      "[[ 4  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Set parameters for the network\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "# Initialize network\n",
    "network = nnet.network(learning_rate, momentum)\n",
    "# Build network architecture\n",
    "network.add_layer(nnet.layer(Xtrain.shape[1],\"sigmoid\")) #input layer\n",
    "network.add_layer(nnet.layer(100,\"sigmoid\"))\n",
    "network.add_layer(nnet.layer(100,\"sigmoid\"))\n",
    "network.add_layer(nnet.layer(100,\"sigmoid\"))\n",
    "network.add_layer(nnet.layer(ytrain.shape[1],\"sigmoid\")) #output layer\n",
    "# Train network\n",
    "n_epochs = 100\n",
    "network.train(X,y,n_epochs)\n",
    "acc_train, ypred = _test(network, Xtrain, ytrain, verbose=True)\n",
    "cm = confusion_matrix([np.argmax(l) for l in ytrain],ypred)\n",
    "print(cm)\n",
    "\n",
    "# Test on cross validation set\n",
    "acc_cv, ypred = _test(network, Xcv, ycv, verbose=True)\n",
    "cm = confusion_matrix([np.argmax(l) for l in ycv],ypred)\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Keras implementation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement the same architecture with keras (three hidden layers with 100 units each and sigmoid activation function). We also use the same momentum and learning rate and stochastic gradient descent optimizer with a mean squared error loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset: 95.83 %\n",
      "[[46  0  0]\n",
      " [ 0 32  5]\n",
      " [ 0  0 37]]\n",
      "Accuracy on dataset: 93.33 %\n",
      "[[ 4  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy on a given dataset\n",
    "def _test_keras(nnet, X, y, verbose=False):\n",
    "    ypred = [np.argmax(i) for i in nnet.predict(X, batch_size=1)]\n",
    "    y = [np.argmax(i) for i in y]\n",
    "    success = np.nansum([1 for i in range(len(y)) if y[i] == ypred[i]])\n",
    "    if verbose:\n",
    "        print(\"Accuracy on dataset: %5.2f %%\" % (100*np.divide(success,len(ypred))))\n",
    "    return 100*np.divide(success,len(ypred)),ypred\n",
    "\n",
    "# Keras implementation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "nnet = Sequential()\n",
    "# Hidden layers\n",
    "nnet.add(Dense(100, input_dim=Xtrain.shape[1], activation='sigmoid'))\n",
    "nnet.add(Dense(100, activation='sigmoid'))\n",
    "nnet.add(Dense(100, activation='sigmoid'))\n",
    "# Output layer\n",
    "nnet.add(Dense(3, activation='sigmoid'))\n",
    "# Set optimizer parameters\n",
    "sgd = SGD(decay=0.0, lr=0.1, nesterov=False, momentum=0.9)\n",
    "# Compile and set the loss function\n",
    "nnet.compile(loss=\"mean_squared_error\", optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Train online for 10 epochs\n",
    "nnet.fit(Xtrain,ytrain,epochs=100,shuffle=True, verbose=0, batch_size=1)\n",
    "acc_train, ypred = _test_keras(nnet, Xtrain, ytrain, verbose=True)\n",
    "cm = confusion_matrix([np.argmax(l) for l in ytrain],ypred)\n",
    "print(cm)\n",
    "# Test on cross validation set\n",
    "acc_cv, ypred = _test_keras(nnet, Xcv, ycv, verbose=True)\n",
    "cm = confusion_matrix([np.argmax(l) for l in ycv],ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results using the two implementations are very similar. Discrepancies are due to different initializations and random shuffling of the data. For a more robust comparison one could use K-fold cross-validation or bootstrapping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
